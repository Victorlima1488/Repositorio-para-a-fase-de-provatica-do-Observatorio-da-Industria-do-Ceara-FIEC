{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, unix_timestamp, floor, format_string, trim, abs, when, year as pyspark_year, month as pyspark_month\n",
    ")\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark_session(app_name):\n",
    "    \"\"\"\n",
    "    Initializes a Spark session.\n",
    "    \"\"\"\n",
    "    return SparkSession.builder.master('local[*]').appName(app_name).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_zip(zip_url, save_dir):\n",
    "    \"\"\"\n",
    "    Downloads a ZIP file, extracts it, and saves the text file inside the specified directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with requests.Session() as session:\n",
    "            response = session.get(zip_url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            zip_file = io.BytesIO(response.content)\n",
    "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                extracted_files = zip_ref.namelist()\n",
    "                print(f\"Extracted files: {extracted_files}\")\n",
    "\n",
    "                txt_file = next((f for f in extracted_files if f.endswith('.txt')), None)\n",
    "\n",
    "                if not txt_file:\n",
    "                    print(\"No TXT file found.\")\n",
    "                    return None\n",
    "\n",
    "                txt_file_path = os.path.join(save_dir, txt_file)\n",
    "                with zip_ref.open(txt_file) as file:\n",
    "                    with open(txt_file_path, 'wb') as out_file:\n",
    "                        out_file.write(file.read())\n",
    "\n",
    "            return txt_file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date_columns(df, date_columns):\n",
    "    \"\"\"\n",
    "    Converts string date columns to timestamp format.\n",
    "    \"\"\"\n",
    "    for column in date_columns:\n",
    "        if column in df.columns:\n",
    "            df = df.withColumn(column, to_timestamp(trim(col(column)), \"dd/MM/yyyy HH:mm:ss\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_time_difference(df, col1, col2, new_col):\n",
    "    \"\"\"\n",
    "    Calculates the time difference between two date columns and creates a new column with the result.\n",
    "    \"\"\"\n",
    "    if col1 in df.columns and col2 in df.columns:\n",
    "        seconds_diff = abs(unix_timestamp(col(col2)) - unix_timestamp(col(col1)))\n",
    "        df = df.withColumn(\n",
    "            new_col,\n",
    "            when(\n",
    "                col(col1).isNull() | col(col2).isNull(), None\n",
    "            ).otherwise(\n",
    "                format_string(\"%02d:%02d:%02d\", floor(seconds_diff / 3600), floor((seconds_diff % 3600) / 60), floor(seconds_diff % 60))\n",
    "            )\n",
    "        )\n",
    "        return df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dfs_to_parquet(dfs, output_path):\n",
    "    \"\"\"\n",
    "    Saves a dictionary of DataFrames to the specified Parquet path.\n",
    "    Creates directories if they don't exist and cleans up old files.\n",
    "    \"\"\"\n",
    "    for year, df in dfs.items():\n",
    "        # Create the output path for each year if it doesn't exist\n",
    "        year_output_path = os.path.join(output_path, str(year))\n",
    "\n",
    "        # Ensure the directory exists (create if it doesn't exist)\n",
    "        os.makedirs(year_output_path, exist_ok=True)\n",
    "\n",
    "        # Coalesce the DataFrame to a single partition for output\n",
    "        single_partition_df = df.coalesce(1)\n",
    "\n",
    "        # Remove any existing files in the directory\n",
    "        if os.path.exists(year_output_path):\n",
    "            # Only remove files within the directory, avoid removing the directory itself\n",
    "            for file_name in os.listdir(year_output_path):\n",
    "                file_path = os.path.join(year_output_path, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)  # Remove old files\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)  # Remove old directories\n",
    "\n",
    "        # Write the DataFrame as Parquet\n",
    "        single_partition_df.write.mode(\"overwrite\").parquet(year_output_path)\n",
    "\n",
    "        # Rename the output file to match the expected format\n",
    "        for filename in os.listdir(year_output_path):\n",
    "            if filename.startswith(\"part-\") and filename.endswith(\".parquet\"):\n",
    "                part_file_path = os.path.join(year_output_path, filename)\n",
    "                new_file_name = f\"{year}_data.parquet\"\n",
    "                new_file_path = os.path.join(year_output_path, new_file_name)\n",
    "                os.rename(part_file_path, new_file_path)\n",
    "                print(f\"Arquivo renomeado para: {new_file_path}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(antaq_links_download, raw_data_path, spark):\n",
    "    \"\"\"\n",
    "    Downloads and processes the files from the provided links.\n",
    "    \"\"\"\n",
    "    dfs_by_file = {}\n",
    "    data_columns = [\n",
    "        \"Data Atracação\", \"Data Chegada\", \"Data Desatracação\",\n",
    "        \"Data Início Operação\", \"Data Término Operação\"\n",
    "    ]\n",
    "\n",
    "    for link in antaq_links_download:\n",
    "        txt_file_path = download_and_extract_zip(link, raw_data_path)\n",
    "        if txt_file_path:\n",
    "            print(f\"TXT file extracted: {txt_file_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to extract file: {link}\")\n",
    "\n",
    "    # List and filter files\n",
    "    all_raw_data_files = os.listdir(raw_data_path)\n",
    "    all_txt_raw_data_files = sorted([f for f in all_raw_data_files if re.match(r\"202\\dAtracacao\\.txt\", f)])\n",
    "\n",
    "    # Process each file\n",
    "    for file_name in all_txt_raw_data_files:\n",
    "        file_full_path = os.path.join(raw_data_path, file_name)\n",
    "        df_name = os.path.splitext(file_name)[0]\n",
    "        df = spark.read.option(\"delimiter\", \";\").csv(file_full_path, header=True, inferSchema=True)\n",
    "\n",
    "        df_transformed = process_date_columns(df, data_columns)\n",
    "\n",
    "        df_transformed = calculate_time_difference(df_transformed, \"Data Atracação\", \"Data Chegada\", \"TEsperaAtracacao\")\n",
    "        df_transformed = calculate_time_difference(df_transformed, \"Data Chegada\", \"Data Início Operação\", \"TEsperaInicioOp\")\n",
    "        df_transformed = calculate_time_difference(df_transformed, \"Data Início Operação\", \"Data Término Operação\", \"TOperacao\")\n",
    "        df_transformed = calculate_time_difference(df_transformed, \"Data Chegada\", \"Data Desatracação\", \"TEsperaDesatracacao\")\n",
    "        df_transformed = calculate_time_difference(df_transformed, \"Data Atracação\", \"Data Desatracação\", \"TAtracado\")\n",
    "        df_transformed = calculate_time_difference(df_transformed, \"Data Atracação\", \"Data Término Operação\", \"TEstadia\")\n",
    "\n",
    "        dfs_by_file[df_name] = df_transformed\n",
    "\n",
    "    return dfs_by_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_carga_and_atracacao(dfs_by_file, raw_data_path, spark):\n",
    "    \"\"\"\n",
    "    Joins Carga and Atracacao DataFrames and stores them in a dictionary.\n",
    "    \"\"\"\n",
    "    dfs_by_year = {}\n",
    "    atracacao_dfs = {k: v for k, v in dfs_by_file.items() if \"Atracacao\" in k}\n",
    "\n",
    "    all_files = os.listdir(raw_data_path)\n",
    "    load_files = sorted([f for f in all_files if re.match(r\"202\\dCarga\\.txt\", f)])\n",
    "\n",
    "    for load_file in load_files:\n",
    "        year_match = re.search(r\"(202\\d)\", load_file)\n",
    "        if not year_match:\n",
    "            continue\n",
    "        year = year_match.group(1)\n",
    "        atracacao_file = f\"{year}Atracacao\"\n",
    "\n",
    "        if atracacao_file not in atracacao_dfs:\n",
    "            print(f\"Berthing DataFrame for {load_file} not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        carga_df = spark.read.option(\"delimiter\", \";\").csv(os.path.join(raw_data_path, load_file), header=True, inferSchema=True)\n",
    "        atracacao_df = atracacao_dfs[atracacao_file]\n",
    "\n",
    "        if \"Data Início Operação\" in atracacao_df.columns:\n",
    "            atracacao_df = atracacao_df.withColumn(\"Ano_Inicio_Operacao\", pyspark_year(col(\"Data Início Operação\")))\n",
    "            atracacao_df = atracacao_df.withColumn(\"Mes_Inicio_Operacao\", pyspark_month(col(\"Data Início Operação\")))\n",
    "\n",
    "        df_final = carga_df.join(\n",
    "            atracacao_df.select(\"IDAtracacao\", \"Ano_Inicio_Operacao\", \"Mes_Inicio_Operacao\"),\n",
    "            on=\"IDAtracacao\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        dfs_by_year[year + \"Carga\"] = df_final\n",
    "\n",
    "    return dfs_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files: ['2021Atracacao.txt']\n",
      "TXT file extracted: c:\\Users\\Victor Lima\\Documents\\Main Victor\\Portifólio\\FIEC\\src\\etl\\notebooks\\..\\..\\..\\data\\raw_data\\2021Atracacao.txt\n",
      "Extracted files: ['2021Carga.txt']\n",
      "TXT file extracted: c:\\Users\\Victor Lima\\Documents\\Main Victor\\Portifólio\\FIEC\\src\\etl\\notebooks\\..\\..\\..\\data\\raw_data\\2021Carga.txt\n",
      "Extracted files: ['2022Atracacao.txt']\n",
      "TXT file extracted: c:\\Users\\Victor Lima\\Documents\\Main Victor\\Portifólio\\FIEC\\src\\etl\\notebooks\\..\\..\\..\\data\\raw_data\\2022Atracacao.txt\n",
      "Extracted files: ['2022Carga.txt']\n",
      "TXT file extracted: c:\\Users\\Victor Lima\\Documents\\Main Victor\\Portifólio\\FIEC\\src\\etl\\notebooks\\..\\..\\..\\data\\raw_data\\2022Carga.txt\n",
      "Extracted files: ['2023Atracacao.txt']\n",
      "TXT file extracted: c:\\Users\\Victor Lima\\Documents\\Main Victor\\Portifólio\\FIEC\\src\\etl\\notebooks\\..\\..\\..\\data\\raw_data\\2023Atracacao.txt\n",
      "Extracted files: ['2023Carga.txt']\n",
      "TXT file extracted: c:\\Users\\Victor Lima\\Documents\\Main Victor\\Portifólio\\FIEC\\src\\etl\\notebooks\\..\\..\\..\\data\\raw_data\\2023Carga.txt\n",
      "Arquivo renomeado para: c:\\Users\\Victor Lima\\Documents\\Main Victor\\Portifólio\\FIEC\\src\\etl\\notebooks\\..\\..\\..\\data\\processed_data\\Atracao.parquet\\2021Atracacao\\2021Atracacao_data.parquet\n",
      "Arquivo renomeado para: c:\\Users\\Victor Lima\\Documents\\Main Victor\\Portifólio\\FIEC\\src\\etl\\notebooks\\..\\..\\..\\data\\processed_data\\Atracao.parquet\\2022Atracacao\\2022Atracacao_data.parquet\n",
      "Arquivo renomeado para: c:\\Users\\Victor Lima\\Documents\\Main Victor\\Portifólio\\FIEC\\src\\etl\\notebooks\\..\\..\\..\\data\\processed_data\\Atracao.parquet\\2023Atracacao\\2023Atracacao_data.parquet\n",
      "Arquivo renomeado para: c:\\Users\\Victor Lima\\Documents\\Main Victor\\Portifólio\\FIEC\\src\\etl\\notebooks\\..\\..\\..\\data\\processed_data\\Carga.parquet\\2021Carga\\2021Carga_data.parquet\n",
      "Arquivo renomeado para: c:\\Users\\Victor Lima\\Documents\\Main Victor\\Portifólio\\FIEC\\src\\etl\\notebooks\\..\\..\\..\\data\\processed_data\\Carga.parquet\\2022Carga\\2022Carga_data.parquet\n",
      "Arquivo renomeado para: c:\\Users\\Victor Lima\\Documents\\Main Victor\\Portifólio\\FIEC\\src\\etl\\notebooks\\..\\..\\..\\data\\processed_data\\Carga.parquet\\2023Carga\\2023Carga_data.parquet\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Setup Spark session\n",
    "    spark = init_spark_session(\"ETL_ANTAQ\")\n",
    "\n",
    "    # Directory paths\n",
    "    raw_data_path = os.path.join(os.getcwd(), '..', '..', '..', 'data', 'raw_data')\n",
    "    output_path = os.path.join(os.getcwd(), '..', '..', '..', 'data', 'processed_data')\n",
    "\n",
    "    # Download, extract, and process files\n",
    "    antaq_links_download = [\n",
    "        \"https://web3.antaq.gov.br/ea/txt/2021Atracacao.zip\",\n",
    "        \"https://web3.antaq.gov.br/ea/txt/2021Carga.zip\",\n",
    "        \"https://web3.antaq.gov.br/ea/txt/2022Atracacao.zip\",\n",
    "        \"https://web3.antaq.gov.br/ea/txt/2022Carga.zip\",\n",
    "        \"https://web3.antaq.gov.br/ea/txt/2023Atracacao.zip\",\n",
    "        \"https://web3.antaq.gov.br/ea/txt/2023Carga.zip\"\n",
    "    ]\n",
    "\n",
    "    # Process files and return DataFrames\n",
    "    dfs_by_file = process_files(antaq_links_download, raw_data_path, spark)\n",
    "\n",
    "    # Process Carga and Atracacao data\n",
    "    dfs_by_year = process_carga_and_atracacao(dfs_by_file, raw_data_path, spark)\n",
    "\n",
    "    # Save results to Parquet\n",
    "    save_dfs_to_parquet(dfs_by_file, os.path.join(output_path, 'Atracao.parquet'))\n",
    "    save_dfs_to_parquet(dfs_by_year, os.path.join(output_path, 'Carga.parquet'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
